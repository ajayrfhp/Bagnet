{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabble image classifier using deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeatureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Define model that maps (3, Q, Q) to (V)        \n",
    "        '''\n",
    "        super(FeatureModel, self).__init__()\n",
    "        self.l1 = self.get_conv_layer((3, 64, 3))\n",
    "        self.l2 = self.get_conv_layer((64, 64, 3))\n",
    "        self.l3 = self.get_conv_layer((64, 128, 3))\n",
    "        self.l = nn.Sequential(*[self.l1, self.l2, self.l3])\n",
    "        self.fc1 = nn.Linear(128*4*4, 100)\n",
    "\n",
    "    def get_conv_layer(self, param):\n",
    "        return nn.Sequential(*[nn.Conv2d(*param), nn.BatchNorm2d(param[1]), nn.ReLU()])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args\n",
    "            patch - (3, Q, Q)\n",
    "        Returns\n",
    "            representation - (V)\n",
    "        Convolutional neural network that maps patch to a vector\n",
    "        '''\n",
    "        x1 = self.l(x)\n",
    "        return self.fc1(x1.view(-1, 128*4*4))\n",
    "\n",
    "model = FeatureModel()\n",
    "model(torch.randn(1, 3, 10, 10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 20, 20, 10]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdhocNet(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(AdhocNet, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.feature_model = FeatureModel()\n",
    "        self.predictor = nn.Linear(100, 10)\n",
    "            \n",
    "    def img_to_patch(self, img, patch_size):\n",
    "        '''\n",
    "        Args\n",
    "            img - (3, W, H)\n",
    "            patch_size - (Q)\n",
    "        Returns\n",
    "            Patches (W/Q, H/Q, 3, Q, Q)\n",
    "        '''\n",
    "        num_patches = int(img.shape[1] / patch_size)\n",
    "        img1 = torch.stack(torch.split(img, num_patches, dim=2))\n",
    "        img2 = torch.stack(torch.split(img1, num_patches, dim=2))\n",
    "        return img2.permute(3, 4, 2, 0, 1)\n",
    "    \n",
    "   \n",
    "    def patches_to_representations(self, patches):\n",
    "        '''\n",
    "        Args\n",
    "            patches - (W/Q, H/Q, 3, Q, Q)\n",
    "        Returns\n",
    "            representations - (W/Q, H/Q, V)\n",
    "        '''\n",
    "        representations = []\n",
    "        for i in range(patches.shape[0]):\n",
    "            row = []\n",
    "            for j in range(patches.shape[1]):\n",
    "                features = self.feature_model(patches[i,j].unsqueeze(0))\n",
    "                scores = self.predictor(features)[0]\n",
    "                row.append(scores)\n",
    "            representations.append(torch.stack(row))\n",
    "        representations = torch.stack(representations)\n",
    "        return representations\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_patches = torch.stack([self.img_to_patch(img, self.patch_size) for img in x])\n",
    "        batch_representations = torch.stack([ self.patches_to_representations(patches) for patches in batch_patches])\n",
    "        return batch_representations, batch_representations.sum(dim=[1, 2])\n",
    "        \n",
    "\n",
    "model = AdhocNet(10)\n",
    "rep, scores = model(torch.randn(1, 3, 200, 200))\n",
    "rep.shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
